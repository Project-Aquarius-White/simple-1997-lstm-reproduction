#!/usr/bin/env python3
"""
Long Time Lag Experiments - Section 5.2 (Noise-Free and Noisy Sequences)

This implements tasks 2a, 2b, and 2c from the 1997 LSTM paper.

Task Description:
    The network must learn to store and reproduce a target value presented
    at the beginning of a sequence, after a long delay filled with distractor
    inputs.
    
Task Variants:
    2a: Noise-free sequences with local regularities
        - Target symbol at start, then p-1 distractor symbols
        - Distractors have local patterns (same symbol repeated)
        - Network must output target at the end
        
    2b: Noisy sequences without local regularities
        - Same as 2a but distractors are random (no local regularities)
        - Harder because network cannot exploit local patterns
        
    2c: Very long time lags
        - Same as 2b but with much longer sequences (up to 1000 steps)
        - Tests the true long-range capability of LSTM

Paper Criterion:
    "Maximal absolute error of all output units below 0.25
    over 10,000 successive sequences"

Paper Hyperparameters (Section 5.2):
    - Sequence length p: 100 for 2a/2b, up to 1000 for 2c
    - Learning rate: 1.0 (2a/2b), 0.01 (2c)
    - Weight init: [-0.2, 0.2]
    - Input symbols: q+1 symbols (q distractors + 1 EOD marker)
    - Target: First symbol determines class output

Reference: Section 5.2 of Hochreiter & Schmidhuber (1997)
"""

import sys
import argparse
from pathlib import Path
from typing import Tuple, Optional, List
import numpy as np

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from aquarius_lstm.metrics import paper_accuracy_criterion, ExperimentResult


# =============================================================================
# Data Generation
# =============================================================================

def generate_long_lag_2a(
    seq_len: int = 100,
    num_samples: int = 100,
    num_symbols: int = 4,
    seed: Optional[int] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate Task 2a: Noise-free sequences with local regularities.
    
    The sequence structure is:
        [target_symbol] [distractor repeated] ... [distractor repeated] [EOD] -> [target_class]
    
    Distractors are chosen from a set of q symbols, but the SAME distractor
    symbol is repeated throughout the sequence (local regularity).
    
    Args:
        seq_len: Total sequence length p (including target and EOD marker)
        num_samples: Number of sequences to generate
        num_symbols: Number of different target symbols (classes)
        seed: Random seed for reproducibility
    
    Returns:
        X: Input sequences, shape (num_samples, seq_len, num_symbols + num_symbols + 1)
           - First num_symbols dims: target symbol one-hot
           - Next num_symbols dims: distractor symbol one-hot
           - Last dim: EOD (End-Of-Distractor) marker
        Y: Target outputs, shape (num_samples, num_symbols)
           - One-hot encoding of the target class
    
    Paper reference:
        "During each sequence presentation, only one distractor symbol was used"
        (local regularity that can be exploited)
    """
    if seed is not None:
        np.random.seed(seed)
    
    # Input encoding: target symbols + distractor symbols + EOD marker
    # Target symbols and distractors share the same set for simplicity
    input_size = num_symbols + num_symbols + 1  # targets + distractors + EOD
    
    X = np.zeros((num_samples, seq_len, input_size), dtype=np.float32)
    Y = np.zeros((num_samples, num_symbols), dtype=np.float32)
    
    for i in range(num_samples):
        # Choose target symbol (determines output class)
        target_symbol = np.random.randint(0, num_symbols)
        
        # Choose single distractor symbol for entire sequence (local regularity)
        distractor_symbol = np.random.randint(0, num_symbols)
        
        # First position: target symbol
        X[i, 0, target_symbol] = 1.0
        
        # Positions 1 to seq_len-2: same distractor repeated (local regularity)
        for t in range(1, seq_len - 1):
            X[i, t, num_symbols + distractor_symbol] = 1.0
        
        # Last position: EOD marker
        X[i, seq_len - 1, -1] = 1.0
        
        # Target output: one-hot of target symbol
        Y[i, target_symbol] = 1.0
    
    return X, Y


def generate_long_lag_2b(
    seq_len: int = 100,
    num_samples: int = 100,
    num_symbols: int = 4,
    seed: Optional[int] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate Task 2b: Noisy sequences without local regularities.
    
    Same as Task 2a, but distractors are randomly sampled at each timestep,
    eliminating local regularities that could be exploited.
    
    Args:
        seq_len: Total sequence length p
        num_samples: Number of sequences to generate
        num_symbols: Number of different symbols
        seed: Random seed for reproducibility
    
    Returns:
        X: Input sequences, shape (num_samples, seq_len, num_symbols + num_symbols + 1)
        Y: Target outputs, shape (num_samples, num_symbols)
    
    Paper reference:
        "This prevents exploitation of local regularities"
    """
    if seed is not None:
        np.random.seed(seed)
    
    input_size = num_symbols + num_symbols + 1
    
    X = np.zeros((num_samples, seq_len, input_size), dtype=np.float32)
    Y = np.zeros((num_samples, num_symbols), dtype=np.float32)
    
    for i in range(num_samples):
        # Choose target symbol
        target_symbol = np.random.randint(0, num_symbols)
        
        # First position: target symbol
        X[i, 0, target_symbol] = 1.0
        
        # Positions 1 to seq_len-2: random distractors at each step
        for t in range(1, seq_len - 1):
            distractor = np.random.randint(0, num_symbols)
            X[i, t, num_symbols + distractor] = 1.0
        
        # Last position: EOD marker
        X[i, seq_len - 1, -1] = 1.0
        
        # Target output
        Y[i, target_symbol] = 1.0
    
    return X, Y


def generate_long_lag_2c(
    seq_len: int = 1000,
    num_samples: int = 100,
    num_symbols: int = 4,
    seed: Optional[int] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate Task 2c: Very long time lags (up to 1000 steps).
    
    Same as Task 2b but with much longer sequences. Tests the true
    long-range gradient flow capability of LSTM.
    
    Args:
        seq_len: Total sequence length (up to 1000)
        num_samples: Number of sequences to generate
        num_symbols: Number of different symbols
        seed: Random seed for reproducibility
    
    Returns:
        X: Input sequences, shape (num_samples, seq_len, num_symbols + num_symbols + 1)
        Y: Target outputs, shape (num_samples, num_symbols)
    
    Paper reference:
        "In case of Task 2c, minimal time lags are in [100, 1000]"
    """
    # Task 2c uses the same generation as 2b, just with longer sequences
    return generate_long_lag_2b(seq_len, num_samples, num_symbols, seed)


def get_data_generator(task: str):
    """Get the appropriate data generator for the task."""
    generators = {
        "2a": generate_long_lag_2a,
        "2b": generate_long_lag_2b,
        "2c": generate_long_lag_2c,
    }
    if task not in generators:
        raise ValueError(f"Unknown task: {task}. Available: {list(generators.keys())}")
    return generators[task]


# =============================================================================
# Paper Hyperparameters
# =============================================================================

def get_paper_hyperparameters(task: str) -> dict:
    """Get paper-specified hyperparameters for each task variant."""
    if task in ["2a", "2b"]:
        return {
            "learning_rate": 1.0,
            "init_range": 0.2,
            "seq_len": 100,
            "num_symbols": 4,
            "hidden_size": 4,
        }
    elif task == "2c":
        return {
            "learning_rate": 0.01,
            "init_range": 0.2,
            "seq_len": 1000,
            "num_symbols": 4,
            "hidden_size": 4,
        }
    else:
        raise ValueError(f"Unknown task: {task}")


# =============================================================================
# Tinygrad Runner
# =============================================================================

def run_tinygrad(
    task: str = "2a",
    seq_len: int = 100,
    num_samples: int = 1000,
    num_epochs: int = 100,
    num_symbols: int = 4,
    hidden_size: int = 4,
    learning_rate: float = 1.0,
    init_range: float = 0.2,
    seed: Optional[int] = 42,
    realize_every: int = 10,
    log_every: int = 10,
    eval_sequences: int = 10000,
) -> ExperimentResult:
    """
    Run the Long Time Lag experiment with tinygrad.
    
    Args:
        task: Task variant ("2a", "2b", or "2c")
        seq_len: Sequence length
        num_samples: Number of training samples
        num_epochs: Number of training epochs
        num_symbols: Number of symbol classes
        hidden_size: LSTM hidden state size
        learning_rate: Learning rate (1.0 for 2a/2b, 0.01 for 2c)
        init_range: Weight initialization range (0.2 per paper)
        seed: Random seed
        realize_every: Realize tensors every N steps
        log_every: Log every N epochs
        eval_sequences: Number of sequences for final evaluation
    
    Returns:
        ExperimentResult with pass/fail status
    """
    from tinygrad.tensor import Tensor
    from aquarius_lstm.cell import LSTMCell1997
    from aquarius_lstm.initialization import init_weights_paper
    
    print(f"\n{'='*60}")
    print(f"LONG TIME LAG - Task {task} (Section 5.2) - tinygrad")
    print(f"{'='*60}")
    print(f"Task: {task}")
    print(f"Sequence length: {seq_len}")
    print(f"Samples: {num_samples}")
    print(f"Hidden size: {hidden_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Init range: [-{init_range}, {init_range}]")
    print(f"{'='*60}\n")
    
    # Input size: target symbols + distractor symbols + EOD marker
    input_size = num_symbols + num_symbols + 1
    output_size = num_symbols
    
    # Generate training data
    data_gen = get_data_generator(task)
    X, Y = data_gen(seq_len, num_samples, num_symbols, seed)
    X_tensor = Tensor(X)
    Y_tensor = Tensor(Y)
    
    # Initialize model with paper-accurate settings
    if seed is not None:
        np.random.seed(seed)
    
    cell = LSTMCell1997(
        input_size=input_size,
        hidden_size=hidden_size,
        init_range=init_range,
        input_gate_bias=0.0,  # Paper doesn't specify, use neutral
        output_gate_bias=0.0,
        seed=seed,
    )
    
    # Output projection layer
    W_out = Tensor(
        init_weights_paper((output_size, hidden_size), init_range, seed),
        requires_grad=True
    )
    b_out = Tensor(np.zeros(output_size, dtype=np.float32), requires_grad=True)
    
    params = cell.parameters() + [W_out, b_out]
    
    # Training loop
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        
        for sample_idx in range(num_samples):
            # Initialize state
            h, s_c = cell.init_state()
            
            # Forward through sequence
            for t in range(seq_len):
                x_t = X_tensor[sample_idx, t]
                h, s_c = cell(x_t, h, s_c)
                
                # Realize periodically to prevent graph explosion
                if t % realize_every == 0:
                    h = h.realize()
                    s_c = s_c.realize()
            
            # Compute prediction and loss (at sequence end)
            pred = W_out.dot(h) + b_out
            pred = pred.sigmoid()  # Output in [0, 1]
            target = Y_tensor[sample_idx]
            loss = ((pred - target) ** 2).sum()
            
            # Backward and update
            loss.backward()
            
            for p in params:
                if p.grad is not None:
                    p.assign(p - p.grad * learning_rate).realize()
                    p.grad = None
            
            epoch_loss += float(loss.numpy())
        
        avg_loss = epoch_loss / num_samples
        
        if (epoch + 1) % log_every == 0:
            print(f"Epoch {epoch+1:3d}/{num_epochs}: Loss = {avg_loss:.6f}")
    
    # Final evaluation on eval_sequences
    print(f"\nEvaluating on {eval_sequences} sequences...")
    X_eval, Y_eval = data_gen(seq_len, eval_sequences, num_symbols, seed=seed+1000 if seed else None)
    X_eval_tensor = Tensor(X_eval)
    
    predictions = []
    for sample_idx in range(eval_sequences):
        h, s_c = cell.init_state()
        for t in range(seq_len):
            x_t = X_eval_tensor[sample_idx, t]
            h, s_c = cell(x_t, h, s_c)
        pred = W_out.dot(h) + b_out
        pred = pred.sigmoid()
        predictions.append(pred.numpy())
    
    predictions = np.array(predictions)
    
    # Evaluate against paper criterion
    result = paper_accuracy_criterion("long_lag", predictions, Y_eval, num_sequences=eval_sequences)
    print(f"\n{result}")
    
    return result


# =============================================================================
# PyTorch Runner
# =============================================================================

def run_torch(
    task: str = "2a",
    seq_len: int = 100,
    num_samples: int = 1000,
    num_epochs: int = 100,
    num_symbols: int = 4,
    hidden_size: int = 4,
    learning_rate: float = 1.0,
    init_range: float = 0.2,
    seed: Optional[int] = 42,
    log_every: int = 10,
    eval_sequences: int = 10000,
) -> ExperimentResult:
    """
    Run the Long Time Lag experiment with PyTorch.
    
    Args:
        task: Task variant ("2a", "2b", or "2c")
        seq_len: Sequence length
        num_samples: Number of training samples
        num_epochs: Number of training epochs
        num_symbols: Number of symbol classes
        hidden_size: LSTM hidden state size
        learning_rate: Learning rate (1.0 for 2a/2b, 0.01 for 2c)
        init_range: Weight initialization range
        seed: Random seed
        log_every: Log every N epochs
        eval_sequences: Number of sequences for final evaluation
    
    Returns:
        ExperimentResult with pass/fail status
    """
    import torch
    from aquarius_lstm.cell_torch import LSTMCell1997Torch
    from aquarius_lstm.initialization import init_weights_paper
    
    print(f"\n{'='*60}")
    print(f"LONG TIME LAG - Task {task} (Section 5.2) - PyTorch")
    print(f"{'='*60}")
    print(f"Task: {task}")
    print(f"Sequence length: {seq_len}")
    print(f"Samples: {num_samples}")
    print(f"Hidden size: {hidden_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Init range: [-{init_range}, {init_range}]")
    print(f"{'='*60}\n")
    
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)
    
    input_size = num_symbols + num_symbols + 1
    output_size = num_symbols
    
    # Generate training data
    data_gen = get_data_generator(task)
    X, Y = data_gen(seq_len, num_samples, num_symbols, seed)
    X_tensor = torch.tensor(X)
    Y_tensor = torch.tensor(Y)
    
    # Initialize model
    cell = LSTMCell1997Torch(
        input_size=input_size,
        hidden_size=hidden_size,
        init_range=init_range,
        input_gate_bias=0.0,
        output_gate_bias=0.0,
        seed=seed,
    )
    
    # Output projection
    W_out = torch.nn.Parameter(
        torch.tensor(init_weights_paper((output_size, hidden_size), init_range))
    )
    b_out = torch.nn.Parameter(torch.zeros(output_size))
    
    params = list(cell.parameters()) + [W_out, b_out]
    optimizer = torch.optim.SGD(params, lr=learning_rate)
    
    # Training loop
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        
        for sample_idx in range(num_samples):
            optimizer.zero_grad()
            
            h, s_c = cell.init_state()
            
            for t in range(seq_len):
                x_t = X_tensor[sample_idx, t]
                h, s_c = cell(x_t, h, s_c)
            
            # Compute prediction at sequence end
            pred = h @ W_out.T + b_out
            pred = torch.sigmoid(pred)
            target = Y_tensor[sample_idx]
            loss = ((pred - target) ** 2).sum()
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / num_samples
        
        if (epoch + 1) % log_every == 0:
            print(f"Epoch {epoch+1:3d}/{num_epochs}: Loss = {avg_loss:.6f}")
    
    # Final evaluation
    print(f"\nEvaluating on {eval_sequences} sequences...")
    X_eval, Y_eval = data_gen(seq_len, eval_sequences, num_symbols, seed=seed+1000 if seed else None)
    X_eval_tensor = torch.tensor(X_eval)
    
    predictions = []
    with torch.no_grad():
        for sample_idx in range(eval_sequences):
            h, s_c = cell.init_state()
            for t in range(seq_len):
                x_t = X_eval_tensor[sample_idx, t]
                h, s_c = cell(x_t, h, s_c)
            pred = h @ W_out.T + b_out
            pred = torch.sigmoid(pred)
            predictions.append(pred.numpy())
    
    predictions = np.array(predictions)
    
    result = paper_accuracy_criterion("long_lag", predictions, Y_eval, num_sequences=eval_sequences)
    print(f"\n{result}")
    
    return result


# =============================================================================
# Main Entry Point
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Long Time Lag Experiments (Section 5.2)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Task Descriptions:
  2a  Noise-free sequences with local regularities (p=100)
  2b  Noisy sequences without local regularities (p=100)
  2c  Very long time lags up to 1000 steps

Paper Hyperparameters:
  - Learning rate: 1.0 (2a/2b), 0.01 (2c)
  - Weight init: [-0.2, 0.2]
  - Criterion: max absolute error < 0.25 over 10,000 sequences
        """
    )
    parser.add_argument("--mode", choices=["smoke", "paper"], default="smoke",
                       help="smoke: quick test, paper: full reproduction")
    parser.add_argument("--backend", choices=["tinygrad", "torch", "both"],
                       default="tinygrad",
                       help="Backend to use for training")
    parser.add_argument("--task", choices=["2a", "2b", "2c", "all"], default="2a",
                       help="Task variant to run")
    parser.add_argument("--seq-len", type=int, default=None,
                       help="Override sequence length")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")
    args = parser.parse_args()
    
    # Determine which tasks to run
    tasks = ["2a", "2b", "2c"] if args.task == "all" else [args.task]
    
    all_results = []
    
    for task in tasks:
        # Get paper hyperparameters for this task
        paper_params = get_paper_hyperparameters(task)
        
        # Set parameters based on mode
        if args.mode == "smoke":
            params = {
                "task": task,
                "seq_len": args.seq_len or min(50, paper_params["seq_len"]),
                "num_samples": 100,
                "num_epochs": 30,
                "num_symbols": paper_params["num_symbols"],
                "hidden_size": paper_params["hidden_size"],
                "learning_rate": paper_params["learning_rate"],
                "init_range": paper_params["init_range"],
                "eval_sequences": 1000,
            }
        else:  # paper mode
            params = {
                "task": task,
                "seq_len": args.seq_len or paper_params["seq_len"],
                "num_samples": 10000,
                "num_epochs": 100,
                "num_symbols": paper_params["num_symbols"],
                "hidden_size": paper_params["hidden_size"],
                "learning_rate": paper_params["learning_rate"],
                "init_range": paper_params["init_range"],
                "eval_sequences": 10000,
            }
        
        params["seed"] = args.seed
        
        # Run with selected backend(s)
        if args.backend in ["tinygrad", "both"]:
            try:
                result = run_tinygrad(**params)
                all_results.append(result)
            except ImportError as e:
                print(f"tinygrad not available: {e}")
        
        if args.backend in ["torch", "both"]:
            try:
                result = run_torch(**params)
                all_results.append(result)
            except ImportError as e:
                print(f"PyTorch not available: {e}")
    
    # Summary
    if len(all_results) > 1:
        print(f"\n{'='*60}")
        print("SUMMARY")
        print(f"{'='*60}")
        for result in all_results:
            print(result)
        print(f"{'='*60}")
    
    # Return exit code based on pass/fail
    passed = all(r.passed for r in all_results) if all_results else False
    return 0 if passed else 1


if __name__ == "__main__":
    sys.exit(main())
