# LSTM 1997 Experiments

This document summarizes the 6 core experiments described in Section 5 of the original 1997 LSTM paper. These experiments were designed to test the network's ability to bridge long time lags and learn complex temporal dependencies.

## Experiment Summary Table

| # | Experiment Name | Primary Task | Key Challenge | Target Metric |
|---|-----------------|--------------|---------------|---------------|
| 1 | Embedded Reber Grammar | Learn finite state grammar | Distributed representations | 100% symbol accuracy |
| 2 | Long Time Lag | Predict last symbol from first | Time lags up to 1000 steps | Abs. error < 0.25 |
| 3 | Two-Sequence Problem | Classify based on mixed signals | Noise on same channel | Zero misclassifications |
| 4 | Adding Problem | Sum two marked values | Precision over long periods | Abs. error < 0.04 |
| 5 | Multiplication Problem | Multiply two marked values | Non-integrative solution | Abs. error < 0.04 |
| 6 | Temporal Order | Classify by sequence order | Order-specific extraction | Abs. error < 0.3 |

---

## 1. Embedded Reber Grammar (Section 5.1)
-   **Task:** Predict the next symbol in a sequence generated by an "Embedded Reber Grammar" (a hierarchical finite-state machine).
-   **Input/Output:** 7-dimensional one-hot encoded symbols (B, T, S, X, V, P, E).
-   **Success Criterion:** 100% prediction accuracy (the most active output must correspond to a valid next symbol).
-   **Hyperparameters:** 
    -   Hidden size: 3-4 memory cell blocks (size 1-2).
    -   Weight init: $[-0.2, 0.2]$.
    -   Gate biases: $-1.0$ to $-4.0$.

## 2. Noise-Free and Noisy Sequences (Section 5.2)
-   **Task:** Store a symbol at the beginning of a sequence and predict it at the very end, across many distractor symbols.
-   **Input/Output:** One-hot encoded symbols with long lags (up to 1000 steps).
-   **Success Criterion:** Maximal absolute error of all output units below 0.25 over 10,000 successive sequences.
-   **Hyperparameters:**
    -   Hidden size: Single cell blocks.
    -   Weight init: $[-0.2, 0.2]$.

## 3. Noise and Signal on Same Channel (Section 5.3)
-   **Task:** Binary classification of sequences where the class is determined by the combination of two widely separated relevant signals, mixed with Gaussian noise on the same input line.
-   **Input/Output:** Real-valued input (signal + noise).
-   **Success Criterion:** 
    -   **ST1:** Zero misclassifications on 256 test sequences.
    -   **ST2:** Mean absolute test error below 0.01.
-   **Hyperparameters:**
    -   Input gate biases: $[-1.0, -3.0, -5.0]$.
    -   Output gate biases: $[-2.0, -4.0, -6.0]$.

## 4. Adding Problem (Section 5.4)
-   **Task:** Two numbers in a sequence of $T$ numbers are marked. The network must output the sum of these two numbers at the end of the sequence.
-   **Input/Output:** Pair of values $(v, m)$, where $v \in [0, 1]$ and $m \in \{0, 1\}$ is a marker. Output is scaled sum: $0.5 + (X_1 + X_2)/4$.
-   **Success Criterion:** Absolute error at sequence end below 0.04.
-   **Hyperparameters:**
    -   Sequence length $T$: 100, 500, or 1000.
    -   Learning rate: 0.5.
    -   Input gate biases: $-3.0$ or $-6.0$.

## 5. Multiplication Problem (Section 5.5)
-   **Task:** Similar to the adding problem, but the target is the product of the two marked values ($X_1 \cdot X_2$).
-   **Input/Output:** Same as Adding Problem.
-   **Success Criterion:** Absolute error below 0.04.
-   **Hyperparameters:**
    -   Learning rate: 0.1 (lower learning rate required for non-linear multiplication).
    -   Other settings same as Adding Problem.

## 6. Temporal Order (Section 5.6)
-   **Task:** Classify sequences based on the temporal order of relevant symbols (e.g., distinguishing between $X \dots Y$ and $Y \dots X$).
-   **Input/Output:** 8-dimensional one-hot symbols (including 4 distractors). 4 classes for 2 symbols, 8 classes for 3 symbols.
-   **Success Criterion:** Final absolute error of all output units below 0.3.
-   **Hyperparameters:**
    -   Learning rate: 0.5 (2 symbols) or 0.1 (3 symbols).
    -   Input gate biases: $-2.0$ to $-6.0$.

---

## Implementation in this Repository
Experiments 4, 5, and 6 are fully implemented as runnable scripts in the `experiments/` directory. Evaluation logic for all 6 experiments is available in `src/aquarius_lstm/metrics.py`.
