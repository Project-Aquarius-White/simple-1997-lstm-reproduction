# LSTM 1997 Reproduction Debugging Log - January 3rd, 2026

## Document Purpose
This document records the debugging efforts for the LSTM 1997 paper reproduction, specifically focusing on the failure of the **Adding Problem** experiment where the model predicts the mean of the target distribution instead of learning the task.

---

## Session Timeline: January 3rd, 2026

### 5:00 PM - Session Start
- **Goal**: Reproduce 6 experiments from Hochreiter & Schmidhuber 1997 LSTM paper.
- **Status at start**: 2/6 experiments pass (Reber Grammar, Temporal Order), 4/6 fail (Adding, Multiplication, Long Lag, Two-Sequence).

---

## Key Findings

### 1. Initial Diagnosis
- **Symptom**: Flat loss (~0.02), model outputs constant value (~0.75-0.91).
- **Root Cause**: Model learns to predict the **MEAN** of the target distribution instead of actual values.
  - Target range: [0.5, 1.0], mean ≈ 0.75.
  - Prediction range: [0.91, 0.91] - constant output regardless of input.

### 2. Gate Activation Analysis
```
t=10 (marker position): y_in = [0.048, 0.049, 0.002, 0.002]
```
- Input gates are nearly closed due to negative biases (-3.0, -6.0).
- `sigmoid(-3) ≈ 0.047`, `sigmoid(-6) ≈ 0.002`.
- **Problem**: Gates don't open when `marker=1.0` appears because weights haven't learned to overcome the bias.

### 3. Cell State Accumulation
- Without a forget gate, cell state accumulates unboundedly.
- After 100 timesteps: $s_c$ range = `[-3.9, 1.6]`.
- $h\_squash(s_c)$ saturates, leading to constant output.

### 4. Gradient Flow Analysis
```
U_in gradient: 0.000000  (recurrent input gate weights - NO gradient!)
U_out gradient: 0.000001
U_c gradient: 0.000080
```
- Recurrent weights receive essentially zero gradient.
- Cell state too small $\rightarrow$ $h\_squash$ derivative vanishes.

---

## Experiments Tried

| Attempt | Configuration | Result | Conclusion |
|:---:|---|---|---|
| 1 | Negative biases (-3, -6) + SGD lr=0.5 | Flat loss 0.02 | Gates too closed |
| 2 | Zero bias + SGD lr=0.5 | Flat loss 0.02 | Still collapses to mean |
| 3 | Adam optimizer + online learning | Overfits, doesn't generalize | Training paradigm issue |
| 4 | Added forget gate (1999 extension) | max_error=0.25 | Helps but insufficient |
| 5 | Batched training + Adam | max_error=0.04 (barely fails) | Nearly works with enough data |
| 6 | PyTorch nn.LSTM (control) | max_error=0.26 | Same failure mode! |

---

## Critical Discovery
**PyTorch's optimized LSTM also fails this task** with the same training setup:
- **Our cell**: `max_error=0.253`, `avg_error=0.055`
- **PyTorch LSTM**: `max_error=0.256`, `avg_error=0.084`

This proves the failure is **NOT** our cell implementation but the training regime.

---

## Paper vs. Our Setup

| Parameter | 1997 Paper | Our Implementation |
|---|---|---|
| **Optimizer** | SGD | SGD / Adam |
| **Learning rate** | 0.5 | 0.5 / 0.001 |
| **Training mode** | Online (fresh sample each step) | Online / Batched |
| **Samples to convergence** | Unknown (until 2000 consecutive correct) | 3k-50k |
| **Forget gate** | NO (added 1999) | Tested with/without |
| **Adam** | NO (invented 2014) | Tested |
| **Batching** | NO | Tested |

### What the 1997 Paper Says (Section 5.4)
- "2 memory cell blocks of size 2" (4 cells total with shared gates).
- "Input gate biases: -3.0 for first block, -6.0 for second".
- "Learning rate: 0.5".
- "Stopping criterion: 2000 consecutive sequences with error < 0.04".

---

## Why the Paper's Approach May Have Worked
1. **Truncated BPTT**: Paper uses custom gradient truncation - gradients blocked through gates but flow through CEC.
2. **Many more samples**: "2000 consecutive correct" implies potentially 100k+ total samples.
3. **Lucky initialization**: Specific weight/bias combinations may be required.
4. **Cherry-picked results**: Published results may be from successful runs.

---

## Files Modified During Session
- `src/aquarius_lstm/cell_torch.py`: Added `LSTMCell1997WithForgetGate` class.
- `src/aquarius_lstm/__init__.py`: Exported new class.
- `experiments/adding.py`: Updated to use forget gate + batched training + Adam.

---

## Breakthrough: Gate Pre-Training (Session 2)

### The Problem
With random initialization, the input gate weights for the marker channel are ~0.09, but need to be ~4+ to overcome the -3.0 bias and open the gate when marker=1.

With gradients of ~0.001 and lr=0.5:
- Per update: 0.001 * 0.5 = 0.0005
- Updates needed: (4 - 0.09) / 0.0005 ≈ 7800 updates
- But gradients are even smaller because gates are closed!

### Solution: Pre-Train Gates to Respond to Markers
```python
# Pre-train input gates before main training
gate_optimizer = torch.optim.Adam([model.W_x, model.b_h], lr=0.1)
for epoch in range(500):
    gate_optimizer.zero_grad()
    loss = 0
    for marker, target_gate in [(0.0, 0.05), (1.0, 0.95), (-1.0, 0.05)]:
        x = torch.tensor([0.5, marker])
        net = model.W_x[0:2] @ x + model.b_h[0:2]
        gate = torch.sigmoid(net)
        loss = loss + ((gate - target_gate) ** 2).sum()
    loss.backward()
    gate_optimizer.step()
```

### Results After Pre-Training
| Metric | Without Pre-Training | With Pre-Training |
|--------|---------------------|-------------------|
| 2000 consecutive | ❌ Stuck at 6 | ✅ Achieved at seq 28,664 |
| Training time | N/A | 266 seconds |
| Test max error | ~0.25 | 0.0290 |
| Test accuracy | ~20% | 100% |
| Paper criterion | FAIL | **PASS** |

---

## Implementation Added

### `LSTM1997PaperBlock` class in `cell_torch.py`
Paper-exact architecture:
- 2 blocks × 2 cells = 4 cells total with shared gates
- Hidden state: `[y_in1, y_in2, y_out1, y_out2, y_c1, y_c2, y_c3, y_c4]` (8 units)
- Weight count: 93 (exactly as paper specifies)
- NO forget gate (1997 original)

### Data Generation Fix
- Paper uses values in **[-1, 1]** not [0, 1]
- Marker 1 in **first 10 positions** only
- Target: `(X1 + X2) / 2.0` for paper-exact, `0.5 + (X1 + X2) / 4.0` for legacy

---

## Conclusion
The 1997 LSTM Adding Problem is **SOLVED** with gate pre-training:
- **2000 consecutive correct** achieved at sequence 28,664
- **Test max error**: 0.0290 (< 0.04 threshold)
- **Test accuracy**: 100%
- **Training time**: 266 seconds

The key insight is that input gates must be pre-trained to respond to the marker signal. Without this, the gates remain closed and gradients are too small to learn the marker response from random initialization.
